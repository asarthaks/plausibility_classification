{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd0c000-4e3f-45e8-a818-d956f54a906b",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a28baba-703d-46b5-9083-d6a2a801d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c72ea1e8-bbb5-4805-989a-78c496dabc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6e8d3b-b1f0-4abc-8713-58fd5c880513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0328d790-8750-4766-bc92-ac62e300806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../datasets/adept/train-dev-test-split/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls ../datasets/adept/train-dev-test-split/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a157adb2-742a-43ac-96c9-f4df0624229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7e17f4-6b02-488a-a4c3-06084df5a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "adept_data_path = \"../../datasets/adept/train-dev-test-split\"\n",
    "split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1565a805-ea81-4d99-93c2-44c9343c89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('{}/{}.json'.format(adept_data_path, split), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89e5623-8fe8-4815-a11b-1f3368b732c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>modifier</th>\n",
       "      <th>noun</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The effect of sleeping is rejuvenation.</td>\n",
       "      <td>The effect of additional sleeping is rejuvenat...</td>\n",
       "      <td>additional</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>3</td>\n",
       "      <td>13484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A toothbrush is for fresh breath.</td>\n",
       "      <td>A regular toothbrush is for fresh breath.</td>\n",
       "      <td>regular</td>\n",
       "      <td>toothbrush</td>\n",
       "      <td>2</td>\n",
       "      <td>2620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A scene is painted.</td>\n",
       "      <td>A negative scene is painted.</td>\n",
       "      <td>negative</td>\n",
       "      <td>scene</td>\n",
       "      <td>2</td>\n",
       "      <td>3324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A bone breaks a tooth.</td>\n",
       "      <td>An alleged bone breaks a tooth.</td>\n",
       "      <td>alleged</td>\n",
       "      <td>bone</td>\n",
       "      <td>2</td>\n",
       "      <td>10610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A trip causes a happening.</td>\n",
       "      <td>A fabulous trip causes a happening.</td>\n",
       "      <td>fabulous</td>\n",
       "      <td>trip</td>\n",
       "      <td>2</td>\n",
       "      <td>14917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 sentence1  \\\n",
       "0  The effect of sleeping is rejuvenation.   \n",
       "1        A toothbrush is for fresh breath.   \n",
       "2                      A scene is painted.   \n",
       "3                   A bone breaks a tooth.   \n",
       "4               A trip causes a happening.   \n",
       "\n",
       "                                           sentence2    modifier        noun  \\\n",
       "0  The effect of additional sleeping is rejuvenat...  additional    sleeping   \n",
       "1          A regular toothbrush is for fresh breath.     regular  toothbrush   \n",
       "2                       A negative scene is painted.    negative       scene   \n",
       "3                    An alleged bone breaks a tooth.     alleged        bone   \n",
       "4                A fabulous trip causes a happening.    fabulous        trip   \n",
       "\n",
       "   label    idx  \n",
       "0      3  13484  \n",
       "1      2   2620  \n",
       "2      2   3324  \n",
       "3      2  10610  \n",
       "4      2  14917  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(train_data)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b9e6b3-0098-4a2d-bbb7-907d165742a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_class_map = {0:\"Impossible\", 1:\"Less Likely\", 2:\"Equally Likely\", 3:\"More Likely\", 4:\"Necessarily True\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a016b8fa-755f-4fab-981c-8032a1e24ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['Impossible', 'Less Likely', 'Equally Likely', 'More Likely', 'Necessarily True'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_class_map.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dd2f6-10a5-4910-8669-8a719093994f",
   "metadata": {},
   "source": [
    "Preprocessing data to get concreteness scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15c55742-f396-457c-9aaa-b67a42a0011f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>modifier</th>\n",
       "      <th>noun</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence2_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The effect of sleeping is rejuvenation.</td>\n",
       "      <td>The effect of additional sleeping is rejuvenat...</td>\n",
       "      <td>additional</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>3</td>\n",
       "      <td>13484</td>\n",
       "      <td>the effect of additional sleeping is rejuvenation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A toothbrush is for fresh breath.</td>\n",
       "      <td>A regular toothbrush is for fresh breath.</td>\n",
       "      <td>regular</td>\n",
       "      <td>toothbrush</td>\n",
       "      <td>2</td>\n",
       "      <td>2620</td>\n",
       "      <td>a regular toothbrush is for fresh breath</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 sentence1  \\\n",
       "0  The effect of sleeping is rejuvenation.   \n",
       "1        A toothbrush is for fresh breath.   \n",
       "\n",
       "                                           sentence2    modifier        noun  \\\n",
       "0  The effect of additional sleeping is rejuvenat...  additional    sleeping   \n",
       "1          A regular toothbrush is for fresh breath.     regular  toothbrush   \n",
       "\n",
       "   label    idx                             sentence2_preprocessed  \n",
       "0      3  13484  the effect of additional sleeping is rejuvenation  \n",
       "1      2   2620           a regular toothbrush is for fresh breath  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['sentence2_preprocessed'] = df_train['sentence2'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "df_train['sentence2_preprocessed'] = df_train['sentence2_preprocessed'].map(lambda x: x.lower())\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb31ed6b-e734-40f1-ae04-3d644129d764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Bigram</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Conc.SD</th>\n",
       "      <th>Unknown</th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent_known</th>\n",
       "      <th>SUBTLEX</th>\n",
       "      <th>Dom_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roadsweeper</td>\n",
       "      <td>0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>traindriver</td>\n",
       "      <td>0</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  Bigram  Conc.M  Conc.SD  Unknown  Total  Percent_known  \\\n",
       "0  roadsweeper       0    4.85     0.37        1     27           0.96   \n",
       "1  traindriver       0    4.54     0.71        3     29           0.90   \n",
       "\n",
       "   SUBTLEX Dom_Pos  \n",
       "0        0       0  \n",
       "1        0       0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We got this dataset for concreteness of 40k words (https://pubmed.ncbi.nlm.nih.gov/24142837/) from https://web.stanford.edu/class/linguist278/data/\n",
    "concreteness_df = pd.read_csv('../../datasets/concreteness/Concreteness_ratings_Brysbaert_et_al_BRM.csv')\n",
    "concreteness_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784bf63a-ea2e-49b7-b6fa-68222779677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_concreteness_score_map = dict()\n",
    "for idx, row in concreteness_df.iterrows():\n",
    "    row = row.to_dict()\n",
    "    word_to_concreteness_score_map[row['Word']] = row['Conc.M']/5.0 # Normalizing to a scale of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb5eef04-632c-4bea-a8fd-ac1de97ad27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39954"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_concreteness_score_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613ac58-f6c2-43ed-9539-40002cbc531e",
   "metadata": {},
   "source": [
    "Some utility functions to get concreteness scores for the input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6baf8c30-513d-4b9d-97cc-fb571a09e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concreteness_score(word):\n",
    "    \"\"\"\n",
    "    Get the concreteness score of a word based on the Concreteness Ratings dataset.\n",
    "    \"\"\"\n",
    "    # If the word is not found in the dataset, return a default score of 0.5\n",
    "    return round(word_to_concreteness_score_map.get(word, 0.5), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a244c3-783f-4464-8aad-0998ec5a5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_concreteness(text):\n",
    "    \"\"\"\n",
    "    Calculate the concreteness score for a given text.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    concreteness_scores = [get_concreteness_score(word) for word in words]\n",
    "    # Take the average concreteness score of all words in the text\n",
    "    return sum(concreteness_scores) / len(concreteness_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a16f43-02b7-48cb-b573-24f688b9a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_concreteness_sequence(text):\n",
    "    \"\"\"\n",
    "    Calculate the concreteness score for a given text.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    concreteness_scores = [get_concreteness_score(word) for word in words]\n",
    "    concreteness_scores = \" \".join([str(i) for i in concreteness_scores])\n",
    "    # Take the average concreteness score of all words in the text\n",
    "    return concreteness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3cc9123-912a-47f4-8000-7af7e9e272e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concreteness Score: 0.5246666666666667\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"the laws of the world can't stop him\"\n",
    "concreteness_score = calculate_text_concreteness(text)\n",
    "print(f\"Concreteness Score: {concreteness_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0e242c3-0be5-4be5-934d-ce2adf258063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concreteness Score: 0.868\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"car crash\"\n",
    "concreteness_score = calculate_text_concreteness(text)\n",
    "print(f\"Concreteness Score: {concreteness_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a8ee2-b88a-4e2c-a746-e80939fdbc38",
   "metadata": {},
   "source": [
    "We are using concreteness score sequence because in the EDA we found that concreteness score can be a usefull factor in distinguishing whether the sentence is plausible or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92b98d4a-534f-4558-b9f0-b7525a3e0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['concreteness_score_sequence'] = df_train.sentence2_preprocessed.apply(calculate_text_concreteness_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f8be043-4951-4da4-8325-5c977a806d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>modifier</th>\n",
       "      <th>noun</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence2_preprocessed</th>\n",
       "      <th>concreteness_score_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The effect of sleeping is rejuvenation.</td>\n",
       "      <td>The effect of additional sleeping is rejuvenat...</td>\n",
       "      <td>additional</td>\n",
       "      <td>sleeping</td>\n",
       "      <td>3</td>\n",
       "      <td>13484</td>\n",
       "      <td>the effect of additional sleeping is rejuvenation</td>\n",
       "      <td>0.286 0.36 0.334 0.486 0.846 0.318 0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A toothbrush is for fresh breath.</td>\n",
       "      <td>A regular toothbrush is for fresh breath.</td>\n",
       "      <td>regular</td>\n",
       "      <td>toothbrush</td>\n",
       "      <td>2</td>\n",
       "      <td>2620</td>\n",
       "      <td>a regular toothbrush is for fresh breath</td>\n",
       "      <td>0.292 0.48 1.0 0.318 0.326 0.394 0.872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 sentence1  \\\n",
       "0  The effect of sleeping is rejuvenation.   \n",
       "1        A toothbrush is for fresh breath.   \n",
       "\n",
       "                                           sentence2    modifier        noun  \\\n",
       "0  The effect of additional sleeping is rejuvenat...  additional    sleeping   \n",
       "1          A regular toothbrush is for fresh breath.     regular  toothbrush   \n",
       "\n",
       "   label    idx                             sentence2_preprocessed  \\\n",
       "0      3  13484  the effect of additional sleeping is rejuvenation   \n",
       "1      2   2620           a regular toothbrush is for fresh breath   \n",
       "\n",
       "               concreteness_score_sequence  \n",
       "0  0.286 0.36 0.334 0.486 0.846 0.318 0.42  \n",
       "1   0.292 0.48 1.0 0.318 0.326 0.394 0.872  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14aeac54-5080-4c85-b8f2-14756ef15a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12892, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bd731-60c4-48c4-afd9-28fbadc78915",
   "metadata": {},
   "source": [
    "Loading validation and test dataset and adding concreteness scores sequences for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92adfbdf-d33c-4f14-bb58-582050d55989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 6) (1612, 6)\n"
     ]
    }
   ],
   "source": [
    "df_validation = pd.DataFrame(json.load(open('{}/{}.json'.format(adept_data_path, \"val\"), 'r')))\n",
    "df_test = pd.DataFrame(json.load(open('{}/{}.json'.format(adept_data_path, \"test\"), 'r')))\n",
    "print(df_validation.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6888b95b-efa9-4b8b-8767-1c5468b51b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation['sentence2_preprocessed'] = df_validation['sentence2'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "df_validation['sentence2_preprocessed'] = df_validation['sentence2_preprocessed'].map(lambda x: x.lower())\n",
    "df_validation['concreteness_score_sequence'] = df_validation.sentence2_preprocessed.apply(calculate_text_concreteness_sequence)\n",
    "\n",
    "df_test['sentence2_preprocessed'] = df_test['sentence2'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "df_test['sentence2_preprocessed'] = df_test['sentence2_preprocessed'].map(lambda x: x.lower())\n",
    "df_test['concreteness_score_sequence'] = df_test.sentence2_preprocessed.apply(calculate_text_concreteness_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f9d85-b90f-4d2a-b138-5d6e2ea71844",
   "metadata": {},
   "source": [
    "### Artificial Dataset Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62119db-637e-44af-8f38-3ab07b82c6f3",
   "metadata": {},
   "source": [
    "Loading artificially created dataset (using Llama 2 70B model). Checkout the notebook modelling/adept/ArtificialDataCreation - ADEPT.ipynb for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c975816-ad8d-4546-8966-5f130a7ba9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artificial_train_combined = pd.read_csv('../../datasets/adept/generated-data/artificial_train_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8970e28d-96ee-4de8-814f-1d2d8f6d9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artificial_train_combined['sentence2_preprocessed'] = df_artificial_train_combined['sentence2'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "df_artificial_train_combined['sentence2_preprocessed'] = df_artificial_train_combined['sentence2_preprocessed'].map(lambda x: x.lower())\n",
    "df_artificial_train_combined['concreteness_score_sequence'] = df_artificial_train_combined.sentence2_preprocessed.apply(calculate_text_concreteness_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a01e590-2796-42c5-b7a9-c5aa53a5c466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>modifier</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence2_preprocessed</th>\n",
       "      <th>concreteness_score_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dog</td>\n",
       "      <td>Happy</td>\n",
       "      <td>A happy dog wags its tail.</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a happy dog wags its tail</td>\n",
       "      <td>0.292 0.512 0.97 0.5 0.38 0.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Building</td>\n",
       "      <td>Tall</td>\n",
       "      <td>A tall building casts a long shadow.</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a tall building casts a long shadow</td>\n",
       "      <td>0.292 0.672 0.928 0.5 0.292 0.636 0.908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       noun modifier                             sentence2  label sentence1  \\\n",
       "0       Dog    Happy            A happy dog wags its tail.      4       NaN   \n",
       "1  Building     Tall  A tall building casts a long shadow.      4       NaN   \n",
       "\n",
       "   idx               sentence2_preprocessed  \\\n",
       "0  NaN            a happy dog wags its tail   \n",
       "1  NaN  a tall building casts a long shadow   \n",
       "\n",
       "               concreteness_score_sequence  \n",
       "0          0.292 0.512 0.97 0.5 0.38 0.992  \n",
       "1  0.292 0.672 0.928 0.5 0.292 0.636 0.908  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_artificial_train_combined.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff13f1-7ab5-4685-95c5-81dc1d261d34",
   "metadata": {},
   "source": [
    "# Fine Tuning Different Transformer Models using different strategies on ADEPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314eb2d5-7462-4b73-9331-8f4287391d6e",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fafe14d4-5175-4bc0-8e2b-9f1f77956e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcb482a7-b289-4fce-9ef5-b9b85f07ace7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../datasets/adept/train-dev-test-split'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adept_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8a051-de82-4c27-8094-078d450701eb",
   "metadata": {},
   "source": [
    "Loading dataset in a format that can be used by huggingface transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a645e70-ca3e-4d9e-b0ac-ab2aaaedd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "adept_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(df_train),\n",
    "    'validation': Dataset.from_pandas(df_validation),\n",
    "    'test': Dataset.from_pandas(df_test),\n",
    "    'artificial_train_combined': Dataset.from_pandas(df_artificial_train_combined)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17a5cad0-543e-4a48-a534-e03ead568db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'modifier', 'noun', 'label', 'idx', 'sentence2_preprocessed', 'concreteness_score_sequence'],\n",
       "        num_rows: 12892\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'modifier', 'noun', 'label', 'idx', 'sentence2_preprocessed', 'concreteness_score_sequence'],\n",
       "        num_rows: 1611\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'modifier', 'noun', 'label', 'idx', 'sentence2_preprocessed', 'concreteness_score_sequence'],\n",
       "        num_rows: 1612\n",
       "    })\n",
       "    artificial_train_combined: Dataset({\n",
       "        features: ['noun', 'modifier', 'sentence2', 'label', 'sentence1', 'idx', 'sentence2_preprocessed', 'concreteness_score_sequence'],\n",
       "        num_rows: 13186\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adept_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af5ce803-b5d5-4f7e-b686-ba2d5562b27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'modifier': Value(dtype='string', id=None),\n",
       " 'noun': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " 'idx': Value(dtype='int64', id=None),\n",
       " 'sentence2_preprocessed': Value(dtype='string', id=None),\n",
       " 'concreteness_score_sequence': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adept_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37de10af-b6c2-4e5b-a42a-d55e530958b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'A year is made up of 365 days.',\n",
       " 'sentence2': 'An outstanding year is made up of 365 days.',\n",
       " 'modifier': 'outstanding',\n",
       " 'noun': 'year',\n",
       " 'label': 2,\n",
       " 'idx': 2825,\n",
       " 'sentence2_preprocessed': 'an outstanding year is made up of 365 days',\n",
       " 'concreteness_score_sequence': '0.292 0.35 0.65 0.318 0.504 0.766 0.334 0.5 0.672'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adept_dataset['train'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93585e59-10ce-4629-a18c-bf8a82b9e075",
   "metadata": {},
   "source": [
    "These are the best params we got after fine tuning different models and parameter using optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cf47ffe-34a4-40be-9e36-cc9e8a36b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.adamw import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e284858-83f2-4b7e-8242-354e3c83d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = ['label', 'input_ids', 'token_type_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e327952-a8c0-4ae9-9515-1660d80ed9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModellingExperiments:\n",
    "    def __init__(self, model_name, dataset, batch_size, learning_rate):\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5, ignore_mismatched_sizes=True)\n",
    "        self.cols_to_keep = set(COLUMNS_TO_KEEP)\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.data_collator = DataCollatorWithPadding(self.tokenizer)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def tokenize_sentence_with_concreteness_score(self, item):\n",
    "        # We also tried using the concreteness score for the whole sentence as a feature input\n",
    "        # To implement that, we changed the source code of transformers library and changed the classification head manually \n",
    "        # So that we can accomodate that extra feature, but this method of encoding concreteness score sequence was giving better score\n",
    "        # Hence, we are using this only in the final experiments. You can check that experiment out in the following notebook:\n",
    "        # modelling/adept/experiments/FinalModellingWithConcretenessScore[DeBERTa] - ADEPT\n",
    "        return self.tokenizer(item['sentence2'], item['concreteness_score_sequence'], truncation=True)\n",
    "        \n",
    "    def tokenize_sentence(self, item):\n",
    "        # Normal tokenization\n",
    "        return self.tokenizer(item['sentence2'], truncation=True)\n",
    "        \n",
    "    def add_strategy_to_tokenizer_function_map(self):\n",
    "        # Mapping between strategy and the tokenization functions defined above\n",
    "        # Strategy refers to whether we are using normal tokenization or whether we want to do paired tokenization of \n",
    "        # both input sentence and the sequence of concreteness score for that sentence\n",
    "        self.strategy_to_tokenizer_function_map = dict()\n",
    "        self.strategy_to_tokenizer_function_map['normal_finetuning'] = self.tokenize_sentence_with_concreteness_score\n",
    "        self.strategy_to_tokenizer_function_map['concreteness_score_addition'] = self.tokenize_sentence\n",
    "        \n",
    "    def prepare_dataset(self, strategy):\n",
    "        # Here, we wull tokenize the dataset based on the strategy we are planning to use\n",
    "        self.strategy = strategy\n",
    "        self.add_strategy_to_tokenizer_function_map()\n",
    "        self.tokenized_dataset = self.dataset.map(self.strategy_to_tokenizer_function_map[self.strategy], batched=True)\n",
    "        current_cols = set(list(self.tokenized_dataset['train'].features.keys()))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.remove_columns(list(current_cols - self.cols_to_keep))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "        self.tokenized_dataset = self.tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "    def prepare_dataloaders(self, train_dataset_type):\n",
    "        # Here, we prepare the dataloaders, it also takes an argument named train_dataset_type which specified\n",
    "        # whether we want to use the original training data or the one combined with out artificially created dataset\n",
    "        self.train_dataset_type = train_dataset_type\n",
    "        self.train_dataloader = DataLoader(self.tokenized_dataset[self.train_dataset_type], batch_size=self.batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "        self.validation_dataloader = DataLoader(self.tokenized_dataset['validation'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "        self.test_dataloader = DataLoader(self.tokenized_dataset['test'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "\n",
    "    def setup_optimizer(self, num_epochs):\n",
    "        # Setting up optimizer and learning rate scheduler\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.num_training_steps = self.num_epochs*len(self.train_dataloader)\n",
    "        self.learning_rate_scheduler = get_scheduler(\"linear\", optimizer=self.optimizer, num_warmup_steps=0, num_training_steps=self.num_training_steps)\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Training the Model\n",
    "        self.model.train()\n",
    "        progress_bar = tqdm(range(self.num_training_steps))\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch in self.train_dataloader:\n",
    "                batch = {k:v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                # calculating gradients\n",
    "                loss.backward()\n",
    "                # optimizing weights\n",
    "                self.optimizer.step()\n",
    "                # updating learning rate\n",
    "                self.learning_rate_scheduler.step()\n",
    "                # flushing gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # updating progress bar\n",
    "                progress_bar.update(1)\n",
    "\n",
    "    def initialize_metrics(self):\n",
    "        # Initializing evaluation metrics\n",
    "        self.accuracy = evaluate.load('accuracy')\n",
    "        self.precision = evaluate.load('precision')\n",
    "        self.recall = evaluate.load('recall')\n",
    "        self.f1 = evaluate.load('f1')\n",
    "        self.roc_auc =  evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "        self.metrics = [self.accuracy, self.precision, self.recall, self.f1]\n",
    "                \n",
    "    def eval_model(self, dataloader):\n",
    "        # Evaluating the model on different dataloaders\n",
    "        self.initialize_metrics()\n",
    "        self.model.eval()\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch)\n",
    "            # Extract logits and predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Extract probabilities for the positive class\n",
    "            positive_probabilities = probabilities\n",
    "        \n",
    "            # Update metrics for accuracy, precision, recall, and F1\n",
    "            for metric in self.metrics:\n",
    "                metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "        \n",
    "            # Update ROC AUC metric\n",
    "            self.roc_auc.add_batch(prediction_scores=positive_probabilities, references=batch['labels'])\n",
    "        \n",
    "        # # Compute metrics for accuracy, precision, recall, and F1\n",
    "        self.eval_dict = {}\n",
    "        self.eval_dict.update(self.accuracy.compute())\n",
    "        self.eval_dict.update(self.precision.compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.recall.compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.f1.compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.roc_auc.compute(multi_class='ovo', average=\"macro\"))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f9c1a-11f0-45ba-8709-ac39959c10df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d265c6-ca58-4ec7-8680-472dd0e96795",
   "metadata": {},
   "source": [
    "# Running Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f9724-8b9c-4ea4-8155-9a659b5960b8",
   "metadata": {},
   "source": [
    "Defining parameters on which we will run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10dda133-6db0-4dcc-8466-8255285f93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_list = [\"facebook/bart-base\", \"microsoft/deberta-base\"]\n",
    "num_epochs_list = [1, 2, 3, 4, 5]\n",
    "strategies_list = [\"normal_finetuning\", \"concreteness_score_addition\"]\n",
    "train_dataset_type_list = [\"train\", \"artificial_train_combined\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee27ab0-a62d-4285-8f24-dd118968018e",
   "metadata": {},
   "source": [
    "Defining static arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3949b6a3-df79-43dd-affa-2f0366e6c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_args = dict()\n",
    "kw_args[\"dataset\"] = adept_dataset\n",
    "kw_args[\"batch_size\"] = 32\n",
    "# This learning rate was found when we were doing hyperparameter tuning of different models\n",
    "# To check out the hyperparameter tuning, look at the following notebook:\n",
    "# modelling/adept/experiments/FineTuningAndModelSelection - ADEPT.ipynb\n",
    "kw_args[\"learning_rate\"] = 3.660515504756857e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8528cd-6bc7-4ec2-9e37-26e99ccd48df",
   "metadata": {},
   "source": [
    "Running experiments in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1be96-cdf8-4db2-8169-0b2b9f96a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 12892/12892 [00:00<00:00, 28035.38 examples/s]\n",
      "Map: 100%|██████████| 1611/1611 [00:00<00:00, 35893.50 examples/s]\n",
      "Map: 100%|██████████| 1612/1612 [00:00<00:00, 37014.10 examples/s]\n",
      "Map: 100%|██████████| 13186/13186 [00:00<00:00, 38723.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training with the following Configurations: {'model_name': 'facebook/bart-base', 'strategy': 'normal_finetuning', 'train_dataset_type': 'train', 'num_epochs': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/403 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 31%|███       | 125/403 [00:34<01:12,  3.85it/s]"
     ]
    }
   ],
   "source": [
    "result_list = list()\n",
    "for model_name in model_name_list:\n",
    "    # Setting Model Name\n",
    "    kw_args[\"model_name\"] = model_name\n",
    "    # Initializing ModellingExperiments Object\n",
    "    modelling_obj = ModellingExperiments(**kw_args)\n",
    "    for strategy in strategies_list:\n",
    "        # Preparing dataset for a specific strategy\n",
    "        modelling_obj.prepare_dataset(strategy=strategy)\n",
    "        for train_dataset_type in train_dataset_type_list:\n",
    "            # Preparing data loaders\n",
    "            modelling_obj.prepare_dataloaders(train_dataset_type=train_dataset_type)\n",
    "            for num_epochs in num_epochs_list:\n",
    "                # Initializing dictionary for storing results\n",
    "                result_dict = dict()\n",
    "                result_dict[\"model_name\"] = model_name\n",
    "                result_dict[\"strategy\"] = strategy\n",
    "                result_dict[\"train_dataset_type\"] = train_dataset_type\n",
    "                result_dict[\"num_epochs\"] = num_epochs\n",
    "                print(\"Model Training with the following Configurations: {}\".format(result_dict))\n",
    "                # For a specic num_epochs variable, we are setting up the optimizers\n",
    "                modelling_obj.setup_optimizer(num_epochs=num_epochs)\n",
    "                # Now, we are training the model\n",
    "                modelling_obj.train_model()\n",
    "                # Now, we will evaluate the model on validation dataset\n",
    "                modelling_obj.eval_model(modelling_obj.validation_dataloader)\n",
    "                # Storing results on validation set\n",
    "                for k, v in modelling_obj.eval_dict.items():\n",
    "                    result_dict[\"validation_{}\".format(k)] = v\n",
    "                print(\"Validation Set Results: {}\".format(modelling_obj.eval_dict))\n",
    "                # Now, we will evaluate the model on test dataset\n",
    "                modelling_obj.eval_model(modelling_obj.test_dataloader)\n",
    "                # Storing results on test set\n",
    "                for k, v in modelling_obj.eval_dict.items():\n",
    "                    result_dict[\"test_{}\".format(k)] = v\n",
    "                print(\"Test Set Results: {}\".format(modelling_obj.eval_dict))\n",
    "                # Storing all the results in the results_list\n",
    "                result_list.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9053d-8cbc-4a7e-89f2-e064ac0c2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(result_list)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34bc89-efd7-418a-b964-89970ee7827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../../results/FinalResultsADEPT.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
