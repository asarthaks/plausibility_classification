{
    "facebook/bart-base#normal_finetuning#artificial_train_combined#4": {
        "validation_accuracy": 0.643699565487275,
        "validation_precision": 0.4078158378027867,
        "validation_recall": 0.3714991574367441,
        "validation_f1": 0.37942413643294054,
        "validation_roc_auc": 0.7094694719258374,
        "test_accuracy": 0.6606699751861043,
        "test_precision": 0.39161957328717706,
        "test_recall": 0.3554124711134231,
        "test_f1": 0.3627788584683084,
        "test_roc_auc": 0.6641203183198178
    },
    "facebook/bart-base#normal_finetuning#artificial_train_combined#3": {
        "validation_accuracy": 0.6443202979515829,
        "validation_precision": 0.4014313334695102,
        "validation_recall": 0.3672757128073122,
        "validation_f1": 0.37549033959965505,
        "validation_roc_auc": 0.7179233396493088,
        "test_accuracy": 0.6563275434243176,
        "test_precision": 0.38449457856929536,
        "test_recall": 0.348070520880415,
        "test_f1": 0.3542251560617733,
        "test_roc_auc": 0.6719976771167546
    },
    "facebook/bart-base#normal_finetuning#artificial_train_combined#2": {
        "validation_accuracy": 0.6374922408441962,
        "validation_precision": 0.3970551324895589,
        "validation_recall": 0.3770339438923707,
        "validation_f1": 0.37753023888517423,
        "validation_roc_auc": 0.7211757995853288,
        "test_accuracy": 0.651985111662531,
        "test_precision": 0.37518582033404735,
        "test_recall": 0.3448109703586015,
        "test_f1": 0.34946260015539027,
        "test_roc_auc": 0.7017139417660468
    },
    "facebook/bart-base#normal_finetuning#artificial_train_combined#1": {
        "validation_accuracy": 0.6505276225946617,
        "validation_precision": 0.41245720598463603,
        "validation_recall": 0.38374849524384336,
        "validation_f1": 0.3888087695406476,
        "validation_roc_auc": 0.7252366524714283,
        "test_accuracy": 0.6538461538461539,
        "test_precision": 0.37836108987405864,
        "test_recall": 0.34336630186945577,
        "test_f1": 0.35138887445857814,
        "test_roc_auc": 0.723249748312444
    },
    "facebook/bart-base#normal_finetuning#train#5": {
        "validation_accuracy": 0.6443202979515829,
        "validation_precision": 0.4651938362175863,
        "validation_recall": 0.38677388595944656,
        "validation_f1": 0.39939857019678254,
        "validation_roc_auc": 0.7375838637626719,
        "test_accuracy": 0.6606699751861043,
        "test_precision": 0.38048816581436395,
        "test_recall": 0.35863803615612533,
        "test_f1": 0.35898968197783987,
        "test_roc_auc": 0.7092185739089073
    },
    "facebook/bart-base#normal_finetuning#train#4": {
        "validation_accuracy": 0.6554934823091247,
        "validation_precision": 0.4949557879093609,
        "validation_recall": 0.38677221976919207,
        "validation_f1": 0.39468894214579625,
        "validation_roc_auc": 0.7495602529697927,
        "test_accuracy": 0.6687344913151365,
        "test_precision": 0.398888658587701,
        "test_recall": 0.3747093183568867,
        "test_f1": 0.3733396404904814,
        "test_roc_auc": 0.7348235694941784
    },
    "facebook/bart-base#normal_finetuning#train#3": {
        "validation_accuracy": 0.6741154562383612,
        "validation_precision": 0.40324671966109465,
        "validation_recall": 0.37521541094734084,
        "validation_f1": 0.38005029841465604,
        "validation_roc_auc": 0.7612250233045723,
        "test_accuracy": 0.6786600496277916,
        "test_precision": 0.4121060574717655,
        "test_recall": 0.39024483349364575,
        "test_f1": 0.390044938493857,
        "test_roc_auc": 0.7555512708679173
    },
    "facebook/bart-base#normal_finetuning#train#2": {
        "validation_accuracy": 0.7001862197392924,
        "validation_precision": 0.4269763268921699,
        "validation_recall": 0.3564779062470913,
        "validation_f1": 0.37822759430349434,
        "validation_roc_auc": 0.7422814105231013,
        "test_accuracy": 0.716501240694789,
        "test_precision": 0.4514923879952083,
        "test_recall": 0.37240834126916256,
        "test_f1": 0.3958830601356985,
        "test_roc_auc": 0.7492831854264257
    },
    "facebook/bart-base#normal_finetuning#train#1": {
        "validation_accuracy": 0.6983240223463687,
        "validation_precision": 0.39217761830006725,
        "validation_recall": 0.298905695735625,
        "validation_f1": 0.30841952173006604,
        "validation_roc_auc": 0.7141204345725466,
        "test_accuracy": 0.7016129032258065,
        "test_precision": 0.41250762446631806,
        "test_recall": 0.2903007672406948,
        "test_f1": 0.3026078123395044,
        "test_roc_auc": 0.6808668267322877
    },
    "facebook/bart-base#normal_finetuning#artificial_train_combined#5": {
        "validation_accuracy": 0.6703910614525139,
        "validation_precision": 0.4065813486627138,
        "validation_recall": 0.36727203676880127,
        "validation_f1": 0.37312513628197685,
        "validation_roc_auc": 0.7402879297500728,
        "test_accuracy": 0.6761786600496278,
        "test_precision": 0.4152866690442756,
        "test_recall": 0.38130122952059714,
        "test_f1": 0.38523743997361537,
        "test_roc_auc": 0.7576557636220635
    },
    "facebook/bart-base#concreteness_score_addition#train#1": {
        "validation_accuracy": 0.6672873991309746,
        "validation_precision": 0.4335393619135853,
        "validation_recall": 0.38452633958239846,
        "validation_f1": 0.3951979307828015,
        "validation_roc_auc": 0.7470173872109005,
        "test_accuracy": 0.673697270471464,
        "test_precision": 0.40857657396940317,
        "test_recall": 0.38270449752678737,
        "test_f1": 0.38462948459951507,
        "test_roc_auc": 0.7557336851942397
    },
    "facebook/bart-base#concreteness_score_addition#train#2": {
        "validation_accuracy": 0.6660459342023588,
        "validation_precision": 0.4031298723670817,
        "validation_recall": 0.37300461092854487,
        "validation_f1": 0.3774121951531434,
        "validation_roc_auc": 0.7503636588539802,
        "test_accuracy": 0.6805210918114144,
        "test_precision": 0.4094072669124461,
        "test_recall": 0.3815957445799464,
        "test_f1": 0.3847752656513183,
        "test_roc_auc": 0.766820852265975
    },
    "facebook/bart-base#concreteness_score_addition#train#3": {
        "validation_accuracy": 0.6505276225946617,
        "validation_precision": 0.5398061756798029,
        "validation_recall": 0.40665292936385294,
        "validation_f1": 0.42623500653743396,
        "validation_roc_auc": 0.748515764192523,
        "test_accuracy": 0.674317617866005,
        "test_precision": 0.40960546938290554,
        "test_recall": 0.38911391460713035,
        "test_f1": 0.38646375330703164,
        "test_roc_auc": 0.7458521297283571
    },
    "facebook/bart-base#concreteness_score_addition#train#4": {
        "validation_accuracy": 0.643699565487275,
        "validation_precision": 0.4569645216650661,
        "validation_recall": 0.4137225746136126,
        "validation_f1": 0.416726523835998,
        "validation_roc_auc": 0.7401436820086774,
        "test_accuracy": 0.6557071960297767,
        "test_precision": 0.39351484110621865,
        "test_recall": 0.3676021625934466,
        "test_f1": 0.364473340289739,
        "test_roc_auc": 0.7174836881262636
    },
    "facebook/bart-base#concreteness_score_addition#train#5": {
        "validation_accuracy": 0.6828057107386716,
        "validation_precision": 0.4168428171386461,
        "validation_recall": 0.36302369046012395,
        "validation_f1": 0.3775324296420528,
        "validation_roc_auc": 0.738428619859327,
        "test_accuracy": 0.6923076923076923,
        "test_precision": 0.4192593278266835,
        "test_recall": 0.34961096098903455,
        "test_f1": 0.36661436526457136,
        "test_roc_auc": 0.778402233381407
    },
    "facebook/bart-base#concreteness_score_addition#artificial_train_combined#1": {
        "validation_accuracy": 0.686530105524519,
        "validation_precision": 0.4158174261033583,
        "validation_recall": 0.3676641635739951,
        "validation_f1": 0.38207362143445195,
        "validation_roc_auc": 0.7353839671107485,
        "test_accuracy": 0.6854838709677419,
        "test_precision": 0.3991659427119542,
        "test_recall": 0.35250038421123475,
        "test_f1": 0.36508453340901564,
        "test_roc_auc": 0.7643648751701808
    },
    "facebook/bart-base#concreteness_score_addition#artificial_train_combined#2": {
        "validation_accuracy": 0.6517690875232774,
        "validation_precision": 0.39181592160957723,
        "validation_recall": 0.3674217333929274,
        "validation_f1": 0.36709446626874087,
        "validation_roc_auc": 0.7354777375721687,
        "test_accuracy": 0.6643920595533499,
        "test_precision": 0.40625614570502455,
        "test_recall": 0.3674891841487059,
        "test_f1": 0.3677640824994799,
        "test_roc_auc": 0.7401712745314315
    },
    "facebook/bart-base#concreteness_score_addition#artificial_train_combined#3": {
        "validation_accuracy": 0.6443202979515829,
        "validation_precision": 0.41899796313475984,
        "validation_recall": 0.3774428037363139,
        "validation_f1": 0.3809376678958689,
        "validation_roc_auc": 0.7223226669874416,
        "test_accuracy": 0.6352357320099256,
        "test_precision": 0.38106414216114015,
        "test_recall": 0.3361755655382014,
        "test_f1": 0.3359405134257877,
        "test_roc_auc": 0.7261458463348364
    },
    "facebook/bart-base#concreteness_score_addition#artificial_train_combined#4": {
        "validation_accuracy": 0.6300434512725015,
        "validation_precision": 0.4286982044905285,
        "validation_recall": 0.3784483189963824,
        "validation_f1": 0.375602276693079,
        "validation_roc_auc": 0.7135676425141415,
        "test_accuracy": 0.6203473945409429,
        "test_precision": 0.37643806026310883,
        "test_recall": 0.3438735191515187,
        "test_f1": 0.3362970445248513,
        "test_roc_auc": 0.7052788852716412
    },
    "facebook/bart-base#concreteness_score_addition#artificial_train_combined#5": {
        "validation_accuracy": 0.6393544382371198,
        "validation_precision": 0.40869604807452875,
        "validation_recall": 0.3823566793339207,
        "validation_f1": 0.3804440660279188,
        "validation_roc_auc": 0.7070052115292504,
        "test_accuracy": 0.6451612903225806,
        "test_precision": 0.389023777968348,
        "test_recall": 0.35277500400213874,
        "test_f1": 0.35315062241025136,
        "test_roc_auc": 0.6932627815040471
    },
    "microsoft/deberta-base#normal_finetuning#train#1": {
        "validation_accuracy": 0.7039106145251397,
        "validation_precision": 0.3469654375788364,
        "validation_recall": 0.30437199906661283,
        "validation_f1": 0.31199713650376054,
        "validation_roc_auc": 0.7136764680785279,
        "test_accuracy": 0.7121588089330024,
        "test_precision": 0.5464811513831122,
        "test_recall": 0.303802687943645,
        "test_f1": 0.3151902059225962,
        "test_roc_auc": 0.7346249369073105
    },
    "microsoft/deberta-base#normal_finetuning#train#2": {
        "validation_accuracy": 0.6952203600248293,
        "validation_precision": 0.42716197879337275,
        "validation_recall": 0.37399069598560364,
        "validation_f1": 0.39322675411628205,
        "validation_roc_auc": 0.7439592840470473,
        "test_accuracy": 0.7183622828784119,
        "test_precision": 0.4451711338525242,
        "test_recall": 0.38731232830122,
        "test_f1": 0.40928552392366785,
        "test_roc_auc": 0.7855887397460618
    },
    "microsoft/deberta-base#normal_finetuning#train#3": {
        "validation_accuracy": 0.680943513345748,
        "validation_precision": 0.6147806586320991,
        "validation_recall": 0.39886662893442076,
        "validation_f1": 0.43044202837239,
        "validation_roc_auc": 0.7221550908524552,
        "test_accuracy": 0.6947890818858561,
        "test_precision": 0.4123503722561147,
        "test_recall": 0.3825430340474797,
        "test_f1": 0.39106680575765007,
        "test_roc_auc": 0.767964245026282
    },
    "microsoft/deberta-base#normal_finetuning#train#4": {
        "validation_accuracy": 0.6871508379888268,
        "validation_precision": 0.4186095583252888,
        "validation_recall": 0.39262791166246014,
        "validation_f1": 0.4012524015527438,
        "validation_roc_auc": 0.7439463909267664,
        "test_accuracy": 0.6923076923076923,
        "test_precision": 0.4084840373243804,
        "test_recall": 0.38431233931211073,
        "test_f1": 0.3927229802883287,
        "test_roc_auc": 0.7742700765539997
    },
    "microsoft/deberta-base#normal_finetuning#train#5": {
        "validation_accuracy": 0.6902545003103663,
        "validation_precision": 0.5185267517664055,
        "validation_recall": 0.4118983147031622,
        "validation_f1": 0.44386541723253314,
        "validation_roc_auc": 0.6943006349330593,
        "test_accuracy": 0.6985111662531017,
        "test_precision": 0.41656189448328806,
        "test_recall": 0.3820292982679011,
        "test_f1": 0.39505962693176716,
        "test_roc_auc": 0.7146024131316175
    },
    "microsoft/deberta-base#normal_finetuning#artificial_train_combined#1": {
        "validation_accuracy": 0.6679081315952824,
        "validation_precision": 0.3906355599348996,
        "validation_recall": 0.36095773978436513,
        "validation_f1": 0.3705696143572116,
        "validation_roc_auc": 0.7003670147783432,
        "test_accuracy": 0.6799007444168734,
        "test_precision": 0.40171509281678774,
        "test_recall": 0.36915271224533164,
        "test_f1": 0.3814225149534182,
        "test_roc_auc": 0.7193427151103073
    },
    "microsoft/deberta-base#normal_finetuning#artificial_train_combined#2": {
        "validation_accuracy": 0.6790813159528243,
        "validation_precision": 0.5601927168654706,
        "validation_recall": 0.4110456190467889,
        "validation_f1": 0.4419498087675951,
        "validation_roc_auc": 0.6837787765199714,
        "test_accuracy": 0.6786600496277916,
        "test_precision": 0.4055059192894347,
        "test_recall": 0.3719294390636822,
        "test_f1": 0.38219619102447067,
        "test_roc_auc": 0.7141537299372234
    },
    "microsoft/deberta-base#normal_finetuning#artificial_train_combined#3": {
        "validation_accuracy": 0.6672873991309746,
        "validation_precision": 0.42092601075142316,
        "validation_recall": 0.3805201429176444,
        "validation_f1": 0.3922714403721782,
        "validation_roc_auc": 0.6857387686563758,
        "test_accuracy": 0.6817617866004962,
        "test_precision": 0.4031520104081278,
        "test_recall": 0.376082090983103,
        "test_f1": 0.3855609234414048,
        "test_roc_auc": 0.704769560314406
    },
    "microsoft/deberta-base#normal_finetuning#artificial_train_combined#4": {
        "validation_accuracy": 0.6815642458100558,
        "validation_precision": 0.4441755949122873,
        "validation_recall": 0.38126863405362243,
        "validation_f1": 0.39991462380303444,
        "validation_roc_auc": 0.6845153127696051,
        "test_accuracy": 0.6730769230769231,
        "test_precision": 0.38596300930972594,
        "test_recall": 0.35991596737783443,
        "test_f1": 0.3694702941009892,
        "test_roc_auc": 0.7158223330649242
    },
    "microsoft/deberta-base#normal_finetuning#artificial_train_combined#5": {
        "validation_accuracy": 0.675356921166977,
        "validation_precision": 0.4616062688071665,
        "validation_recall": 0.3976541954881011,
        "validation_f1": 0.4171796696172333,
        "validation_roc_auc": 0.6858180539222076,
        "test_accuracy": 0.6755583126550868,
        "test_precision": 0.3912852700047992,
        "test_recall": 0.3557209575066992,
        "test_f1": 0.3666117464465253,
        "test_roc_auc": 0.7083185792693363
    },
    "microsoft/deberta-base#concreteness_score_addition#train#1": {
        "validation_accuracy": 0.6610800744878957,
        "validation_precision": 0.43693302827527064,
        "validation_recall": 0.3730871880389382,
        "validation_f1": 0.3889061622307941,
        "validation_roc_auc": 0.7014696706757467,
        "test_accuracy": 0.6674937965260546,
        "test_precision": 0.38542062193126025,
        "test_recall": 0.35456562191172064,
        "test_f1": 0.3633524424354881,
        "test_roc_auc": 0.7063884687387432
    },
    "microsoft/deberta-base#concreteness_score_addition#train#2": {
        "validation_accuracy": 0.6654252017380509,
        "validation_precision": 0.42096816076888555,
        "validation_recall": 0.38395121238635616,
        "validation_f1": 0.3962121473519412,
        "validation_roc_auc": 0.7033909372979876,
        "test_accuracy": 0.6761786600496278,
        "test_precision": 0.39573171334361845,
        "test_recall": 0.3730877008553174,
        "test_f1": 0.3792386452799055,
        "test_roc_auc": 0.7213488634014896
    },
    "microsoft/deberta-base#concreteness_score_addition#train#3": {
        "validation_accuracy": 0.6561142147734327,
        "validation_precision": 0.4255860396032184,
        "validation_recall": 0.3827421523991988,
        "validation_f1": 0.3917694438485236,
        "validation_roc_auc": 0.7178306207724361,
        "test_accuracy": 0.6650124069478908,
        "test_precision": 0.380317648250368,
        "test_recall": 0.3572426502335766,
        "test_f1": 0.3626975631021918,
        "test_roc_auc": 0.6957536831853407
    },
    "microsoft/deberta-base#concreteness_score_addition#train#4": {
        "validation_accuracy": 0.6691495965238982,
        "validation_precision": 0.4346649988867,
        "validation_recall": 0.38018099555218376,
        "validation_f1": 0.39672404228922226,
        "validation_roc_auc": 0.6860779104855007,
        "test_accuracy": 0.6823821339950372,
        "test_precision": 0.3939163651795911,
        "test_recall": 0.3665468073565523,
        "test_f1": 0.3765325312711787,
        "test_roc_auc": 0.7053148230133509
    },
    "microsoft/deberta-base#concreteness_score_addition#train#5": {
        "validation_accuracy": 0.6697703289882061,
        "validation_precision": 0.4356658790149647,
        "validation_recall": 0.39683173873014665,
        "validation_f1": 0.4100145160750013,
        "validation_roc_auc": 0.6770104379848745,
        "test_accuracy": 0.6650124069478908,
        "test_precision": 0.3765610327776234,
        "test_recall": 0.3552000762130788,
        "test_f1": 0.35959079484898976,
        "test_roc_auc": 0.6861956297100398
    },
    "microsoft/deberta-base#concreteness_score_addition#artificial_train_combined#1": {
        "validation_accuracy": 0.6604593420235878,
        "validation_precision": 0.4115509349635703,
        "validation_recall": 0.3937624750109034,
        "validation_f1": 0.39848980524362415,
        "validation_roc_auc": 0.6914127884447117,
        "test_accuracy": 0.6681141439205955,
        "test_precision": 0.3854999532869029,
        "test_recall": 0.37086706776825995,
        "test_f1": 0.3740629234722924,
        "test_roc_auc": 0.6914842257077026
    },
    "microsoft/deberta-base#concreteness_score_addition#artificial_train_combined#2": {
        "validation_accuracy": 0.6685288640595903,
        "validation_precision": 0.414127816394296,
        "validation_recall": 0.3803838639843709,
        "validation_f1": 0.39357890740135176,
        "validation_roc_auc": 0.6889716928797687,
        "test_accuracy": 0.6761786600496278,
        "test_precision": 0.3837815671209168,
        "test_recall": 0.3587822085385712,
        "test_f1": 0.3675782528218282,
        "test_roc_auc": 0.684431416526113
    },
    "microsoft/deberta-base#concreteness_score_addition#artificial_train_combined#3": {
        "validation_accuracy": 0.6834264432029795,
        "validation_precision": 0.478087844561833,
        "validation_recall": 0.3889682444977398,
        "validation_f1": 0.4164942246940207,
        "validation_roc_auc": 0.6790667838524148,
        "test_accuracy": 0.6861042183622829,
        "test_precision": 0.39861550932887047,
        "test_recall": 0.3530838638799513,
        "test_f1": 0.36885031928029977,
        "test_roc_auc": 0.7056425245341424
    },
    "microsoft/deberta-base#concreteness_score_addition#artificial_train_combined#4": {
        "validation_accuracy": 0.6641837368094351,
        "validation_precision": 0.4126809498784147,
        "validation_recall": 0.3578836287840328,
        "validation_f1": 0.3752672542352229,
        "validation_roc_auc": 0.6892224830898426,
        "test_accuracy": 0.6830024813895782,
        "test_precision": 0.3955779657812998,
        "test_recall": 0.35329457209328885,
        "test_f1": 0.36753432352103765,
        "test_roc_auc": 0.6886548938110679
    },
    "microsoft/deberta-base#concreteness_score_addition#artificial_train_combined#5": {
        "validation_accuracy": 0.6679081315952824,
        "validation_precision": 0.41806778787730836,
        "validation_recall": 0.37698908700488826,
        "validation_f1": 0.39082621614704455,
        "validation_roc_auc": 0.6652193222193417,
        "test_accuracy": 0.6718362282878412,
        "test_precision": 0.389527456759045,
        "test_recall": 0.356901336826667,
        "test_f1": 0.36771529163052563,
        "test_roc_auc": 0.6941670168026562
    }
}