{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Loads PAP datasets\n",
    "datasets_path = '../datasets/pap/train-dev-test-split/binary'\n",
    "train_df = pd.read_csv(f'{datasets_path}/train.csv')\n",
    "dev_df = pd.read_csv(f'{datasets_path}/dev.csv')\n",
    "test_df = pd.read_csv(f'{datasets_path}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads and preprocess concreteness ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got this dataset for concreteness of 40k words (https://pubmed.ncbi.nlm.nih.gov/24142837/) from https://web.stanford.edu/class/linguist278/data/\n",
    "# Load concreteness ratings\n",
    "concreteness_df = pd.read_csv('../datasets/concreteness/Concreteness_ratings_Brysbaert_et_al_BRM.csv')\n",
    "concreteness_df.head(2)\n",
    "\n",
    "# Map and normalize conreteness ratings\n",
    "word_to_concreteness_score_map = dict()\n",
    "for idx, row in concreteness_df.iterrows():\n",
    "    row = row.to_dict()\n",
    "    \n",
    "    # Normalizing to a scale of 0 to 1\n",
    "    word_to_concreteness_score_map[row['Word']] = row['Conc.M']/5.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to get concreteness scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_concreteness_score(word):\n",
    "    \"\"\"\n",
    "    Get the concreteness score of a word based on the Concreteness Ratings dataset.\n",
    "    \"\"\"\n",
    "    # If the word is not found in the dataset, return a default score of 0.5\n",
    "    return round(word_to_concreteness_score_map.get(word, 0.5), 3)\n",
    "\n",
    "def calculate_text_concreteness_sequence(text):\n",
    "    \"\"\"\n",
    "    Calculate the concreteness score for a given text.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    concreteness_scores = [get_concreteness_score(word) for word in words]\n",
    "    concreteness_scores = \" \".join([str(i) for i in concreteness_scores])\n",
    "    # Take the average concreteness score of all words in the text\n",
    "    return concreteness_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PAP datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add concreteness scores for the every sequence\n",
    "train_df['concreteness_score_sequence'] = train_df.text.apply(calculate_text_concreteness_sequence)\n",
    "dev_df['concreteness_score_sequence'] = dev_df.text.apply(calculate_text_concreteness_sequence)\n",
    "test_df['concreteness_score_sequence'] = test_df.text.apply(calculate_text_concreteness_sequence)\n",
    "\n",
    "# Load PAP datasets with Concreteness Scores  \n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(dev_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModellingExperiments:\n",
    "    COLUMNS_TO_KEEP = ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "    def __init__(self, model_name, dataset, batch_size, learning_rate):\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "        self.cols_to_keep = set(COLUMNS_TO_KEEP)\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.data_collator = DataCollatorWithPadding(self.tokenizer)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def tokenize_sentence_with_concreteness_score(self, item):\n",
    "        # We also tried using the concreteness score for the whole sentence as a feature input\n",
    "        # To implement that, we changed the source code of transformers library and changed the classification head manually \n",
    "        # So that we can accomodate that extra feature, but this method of encoding concreteness score sequence was giving better score\n",
    "        # Hence, we are using this only in the final experiments. You can check that experiment out in the following notebook:\n",
    "        # modelling/adept/experiments/FinalModellingWithConcretenessScore[DeBERTa] - ADEPT\n",
    "        return self.tokenizer(item['sentence2'], item['concreteness_score_sequence'], truncation=True)\n",
    "        \n",
    "    def tokenize_sentence(self, item):\n",
    "        # Normal tokenization\n",
    "        return self.tokenizer(item['sentence2'], truncation=True)\n",
    "        \n",
    "    def add_strategy_to_tokenizer_function_map(self):\n",
    "        # Mapping between strategy and the tokenization functions defined above\n",
    "        # Strategy refers to whether we are using normal tokenization or whether we want to do paired tokenization of \n",
    "        # both input sentence and the sequence of concreteness score for that sentence\n",
    "        self.strategy_to_tokenizer_function_map = dict()\n",
    "        self.strategy_to_tokenizer_function_map['normal_finetuning'] = self.tokenize_sentence_with_concreteness_score\n",
    "        self.strategy_to_tokenizer_function_map['concreteness_score_addition'] = self.tokenize_sentence\n",
    "        \n",
    "    def prepare_dataset(self, strategy):\n",
    "        # Here, we wull tokenize the dataset based on the strategy we are planning to use\n",
    "        self.strategy = strategy\n",
    "        self.add_strategy_to_tokenizer_function_map()\n",
    "        self.tokenized_dataset = self.dataset.map(self.strategy_to_tokenizer_function_map[self.strategy], batched=True)\n",
    "        current_cols = set(list(self.tokenized_dataset['train'].features.keys()))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.remove_columns(list(current_cols - self.cols_to_keep))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "        self.tokenized_dataset = self.tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "    def prepare_dataloaders(self):\n",
    "        self.train_dataloader = DataLoader(self.tokenized_dataset['train'], batch_size=self.batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "        self.validation_dataloader = DataLoader(self.tokenized_dataset['validation'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "        self.test_dataloader = DataLoader(self.tokenized_dataset['test'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "\n",
    "    def setup_optimizer(self, num_epochs):\n",
    "        # Setting up optimizer and learning rate scheduler\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.num_training_steps = self.num_epochs*len(self.train_dataloader)\n",
    "        self.learning_rate_scheduler = get_scheduler(\"linear\", optimizer=self.optimizer, num_warmup_steps=0, num_training_steps=self.num_training_steps)\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Training the Model\n",
    "        self.model.train()\n",
    "        progress_bar = tqdm(range(self.num_training_steps))\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for batch in self.train_dataloader:\n",
    "                batch = {k:v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                # calculating gradients\n",
    "                loss.backward()\n",
    "                # optimizing weights\n",
    "                self.optimizer.step()\n",
    "                # updating learning rate\n",
    "                self.learning_rate_scheduler.step()\n",
    "                # flushing gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # updating progress bar\n",
    "                progress_bar.update(1)\n",
    "\n",
    "    def initialize_metrics(self):\n",
    "        # Initializing evaluation metrics\n",
    "        self.accuracy = evaluate.load('accuracy')\n",
    "        self.precision = evaluate.load('precision')\n",
    "        self.recall = evaluate.load('recall')\n",
    "        self.f1 = evaluate.load('f1')\n",
    "        self.roc_auc =  evaluate.load(\"roc_auc\")\n",
    "        self.metrics = [self.accuracy, self.precision, self.recall, self.f1]\n",
    "                \n",
    "    def eval_model(self, dataloader):\n",
    "        # Evaluating the model on different dataloaders\n",
    "        self.initialize_metrics()\n",
    "        self.model.eval()\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch)\n",
    "            # Extract logits and predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Extract probabilities for the positive class\n",
    "            positive_probabilities = probabilities\n",
    "        \n",
    "            # Update metrics for accuracy, precision, recall, and F1\n",
    "            for metric in self.metrics:\n",
    "                metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "        \n",
    "            # Update ROC AUC metric\n",
    "            self.roc_auc.add_batch(prediction_scores=positive_probabilities, references=batch['labels'])\n",
    "        \n",
    "        # # Compute metrics for accuracy, precision, recall, and F1\n",
    "        self.eval_dict = {}\n",
    "        self.eval_dict.update(self.accuracy.compute())\n",
    "        self.eval_dict.update(self.precision.compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.recall.compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.f1.compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.roc_auc.compute(average=\"macro\"))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters on which we will run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "model_name_list = [\"facebook/bart-base\", \"microsoft/deberta-base\"]\n",
    "num_epochs_list = [1, 2, 3, 4, 5]\n",
    "strategies_list = [\"normal_finetuning\", \"concreteness_score_addition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining static arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_args = {\n",
    "    'dataset' = raw_datasets,\n",
    "    'batch_size' = 32,\n",
    "    'learning_rate' = 3e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = list()\n",
    "for model_name in model_name_list:\n",
    "    \n",
    "    # Setting Model Name\n",
    "    kw_args[\"model_name\"] = model_name\n",
    "    \n",
    "    # Initializing ModellingExperiments Object\n",
    "    modelling_obj = ModellingExperiments(**kw_args)\n",
    "    \n",
    "    for strategy in strategies_list:\n",
    "        \n",
    "        # Preparing dataset for a specific strategy\n",
    "        modelling_obj.prepare_dataset(strategy=strategy)\n",
    "        \n",
    "        # Preparing data loaders\n",
    "        modelling_obj.prepare_dataloaders()\n",
    "            \n",
    "        # Training loop\n",
    "        for num_epochs in num_epochs_list:\n",
    "            # Initializing dictionary for storing results\n",
    "            result_dict = dict()\n",
    "            result_dict[\"model_name\"] = model_name\n",
    "            result_dict[\"strategy\"] = strategy\n",
    "            #result_dict[\"train_dataset_type\"] = train_dataset_type\n",
    "            result_dict[\"num_epochs\"] = num_epochs\n",
    "            print(\"Model Training with the following Configurations: {}\".format(result_dict))\n",
    "            \n",
    "            # For a specic num_epochs variable, we are setting up the optimizers\n",
    "            modelling_obj.setup_optimizer(num_epochs=num_epochs)\n",
    "            \n",
    "            # Now, we are training the model\n",
    "            modelling_obj.train_model()\n",
    "            \n",
    "            # Now, we will evaluate the model on validation dataset\n",
    "            modelling_obj.eval_model(modelling_obj.validation_dataloader)\n",
    "            \n",
    "            # Storing results on validation set\n",
    "            for k, v in modelling_obj.eval_dict.items():\n",
    "                result_dict[\"validation_{}\".format(k)] = v\n",
    "            print(\"Validation Set Results: {}\".format(modelling_obj.eval_dict))\n",
    "            \n",
    "            # Now, we will evaluate the model on test dataset\n",
    "            modelling_obj.eval_model(modelling_obj.test_dataloader)\n",
    "            \n",
    "            # Storing results on test set\n",
    "            for k, v in modelling_obj.eval_dict.items():\n",
    "                result_dict[\"test_{}\".format(k)] = v\n",
    "            \n",
    "            print(\"Test Set Results: {}\".format(modelling_obj.eval_dict))\n",
    "            \n",
    "            # Storing all the results in the results_list\n",
    "            result_list.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(result_list)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../../results/FinalResultsPAP.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
