{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAP (Binary) - Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Loads PAP datasets\n",
    "datasets_path = '../../datasets/pap/train-dev-test-split/binary'\n",
    "train_df = pd.read_csv(f'{datasets_path}/train.csv')\n",
    "dev_df = pd.read_csv(f'{datasets_path}/dev.csv')\n",
    "test_df = pd.read_csv(f'{datasets_path}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads and preprocess concreteness ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got this dataset for concreteness of 40k words (https://pubmed.ncbi.nlm.nih.gov/24142837/) from https://web.stanford.edu/class/linguist278/data/\n",
    "# Load concreteness ratings\n",
    "concreteness_df = pd.read_csv('../../datasets/concreteness/Concreteness_ratings_Brysbaert_et_al_BRM.csv')\n",
    "concreteness_df.head(2)\n",
    "\n",
    "# Map and normalize conreteness ratings\n",
    "word_to_concreteness_score_map = dict()\n",
    "for idx, row in concreteness_df.iterrows():\n",
    "    row = row.to_dict()\n",
    "    \n",
    "    # Normalizing to a scale of 0 to 1\n",
    "    word_to_concreteness_score_map[row['Word']] = row['Conc.M']/5.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to get concreteness scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_concreteness_score(word):\n",
    "    \"\"\"\n",
    "    Get the concreteness score of a word based on the Concreteness Ratings dataset.\n",
    "    \"\"\"\n",
    "    # If the word is not found in the dataset, return a default score of 0.5\n",
    "    return round(word_to_concreteness_score_map.get(word, 0.5), 3)\n",
    "\n",
    "def calculate_text_concreteness_sequence(text):\n",
    "    \"\"\"\n",
    "    Calculate the concreteness score for a given text.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    concreteness_scores = [get_concreteness_score(word) for word in words]\n",
    "    concreteness_scores = \" \".join([str(i) for i in concreteness_scores])\n",
    "    # Take the average concreteness score of all words in the text\n",
    "    return concreteness_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PAP datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add concreteness scores for the every sequence\n",
    "train_df['concreteness_score_sequence'] = train_df.text.apply(calculate_text_concreteness_sequence)\n",
    "dev_df['concreteness_score_sequence'] = dev_df.text.apply(calculate_text_concreteness_sequence)\n",
    "test_df['concreteness_score_sequence'] = test_df.text.apply(calculate_text_concreteness_sequence)\n",
    "\n",
    "# Load PAP datasets with Concreteness Scores  \n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(dev_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "class ModellingExperiments:\n",
    "    \n",
    "    def __init__(self, model_name, dataset, batch_size, learning_rate):\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.cols_to_keep = set(COLUMNS_TO_KEEP)\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.data_collator = DataCollatorWithPadding(self.tokenizer)\n",
    "\n",
    "    def init_model(self):\n",
    "        torch.manual_seed(12)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def tokenize_sentence_with_concreteness_score(self, item):\n",
    "        # We also tried using the concreteness score for the whole sentence as a feature input\n",
    "        # To implement that, we changed the source code of transformers library and changed the classification head manually \n",
    "        # So that we can accomodate that extra feature, but this method of encoding concreteness score sequence was giving better score\n",
    "        # Hence, we are using this only in the final experiments. You can check that experiment out in the following notebook:\n",
    "        # modelling/pap/experiments/FinalModellingWithConcretenessScore[BERT] - PAP\n",
    "        return self.tokenizer(item['text'], item['concreteness_score_sequence'], truncation=True)\n",
    "        \n",
    "    def tokenize_sentence(self, item):\n",
    "        # Normal tokenization\n",
    "        return self.tokenizer(item['text'], truncation=True)\n",
    "        \n",
    "    def add_strategy_to_tokenizer_function_map(self):\n",
    "        # Mapping between strategy and the tokenization functions defined above\n",
    "        # Strategy refers to whether we are using normal tokenization or whether we want to do paired tokenization of \n",
    "        # both input sentence and the sequence of concreteness score for that sentence\n",
    "        self.strategy_to_tokenizer_function_map = dict()\n",
    "        self.strategy_to_tokenizer_function_map['normal_finetuning'] = self.tokenize_sentence_with_concreteness_score\n",
    "        self.strategy_to_tokenizer_function_map['concreteness_score_addition'] = self.tokenize_sentence\n",
    "        \n",
    "    def prepare_dataset(self, strategy):\n",
    "        # Here, we wull tokenize the dataset based on the strategy we are planning to use\n",
    "        self.strategy = strategy\n",
    "        self.add_strategy_to_tokenizer_function_map()\n",
    "        self.tokenized_dataset = self.dataset.map(self.strategy_to_tokenizer_function_map[self.strategy], batched=True)\n",
    "        current_cols = set(list(self.tokenized_dataset['train'].features.keys()))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.remove_columns(list(current_cols - self.cols_to_keep))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "        self.tokenized_dataset = self.tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "    def prepare_dataloaders(self):\n",
    "        self.train_dataloader = DataLoader(self.tokenized_dataset['train'], batch_size=self.batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "        self.validation_dataloader = DataLoader(self.tokenized_dataset['validation'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "        self.test_dataloader = DataLoader(self.tokenized_dataset['test'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "\n",
    "    def setup_optimizer(self, num_epochs):\n",
    "        # Setting up optimizer and learning rate scheduler\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.num_training_steps = self.num_epochs*len(self.train_dataloader)\n",
    "        self.learning_rate_scheduler = get_scheduler(\"linear\", optimizer=self.optimizer, num_warmup_steps=0, num_training_steps=self.num_training_steps)\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Training the Model\n",
    "        self.evaluation_results_list = list()\n",
    "        progress_bar = tqdm(range(self.num_training_steps))\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            for batch in self.train_dataloader:\n",
    "                batch = {k:v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                # calculating gradients\n",
    "                loss.backward()\n",
    "                # optimizing weights\n",
    "                self.optimizer.step()\n",
    "                # updating learning rate\n",
    "                self.learning_rate_scheduler.step()\n",
    "                # flushing gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # updating progress bar\n",
    "                progress_bar.update(1)\n",
    "            # evaluating per epoch\n",
    "            self.eval_model(self.validation_dataloader)\n",
    "            eval_results = dict()\n",
    "            eval_results['epoch'] = epoch + 1\n",
    "            for k, v in self.eval_dict.items():\n",
    "                eval_results[\"validation_{}\".format(k)] = v\n",
    "            self.eval_model(self.test_dataloader)\n",
    "            for k, v in self.eval_dict.items():\n",
    "                eval_results[\"test_{}\".format(k)] = v\n",
    "            self.evaluation_results_list.append(eval_results)\n",
    "\n",
    "    def initialize_metrics(self):\n",
    "        self.metrics = {\n",
    "            'accuracy': evaluate.load('accuracy'),\n",
    "            'precision': evaluate.load('precision'),\n",
    "            'recall': evaluate.load('recall'),\n",
    "            'f1': evaluate.load('f1'),\n",
    "            'roc-auc': evaluate.load(\"roc_auc\"),\n",
    "        }\n",
    "                \n",
    "    def eval_model(self, dataloader):\n",
    "        \n",
    "        # Evaluating the model on different dataloaders\n",
    "        self.initialize_metrics()\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Move batch data to the specified device (GPU or CPU)\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "    \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch)\n",
    "            \n",
    "            # Extract logits and predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Extract probabilities for the positive class\n",
    "            positive_probabilities = probabilities[:, 1].to(self.device).numpy()\n",
    "        \n",
    "            # Update metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "            self.metrics['accuracy'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['precision'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['recall'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['f1'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['roc-auc'].add_batch(prediction_scores=positive_probabilities, references=batch['labels'])\n",
    "        \n",
    "        # Compute metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "        self.eval_dict = {}\n",
    "        self.eval_dict.update(self.metrics['accuracy'].compute())\n",
    "        self.eval_dict.update(self.metrics['precision'].compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.metrics['recall'].compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.metrics['f1'].compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.metrics['roc-auc'].compute(average=\"macro\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters on which we will run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "model_name_list = [\"facebook/bart-base\", \"microsoft/deberta-base\"]\n",
    "num_epochs = 4\n",
    "strategies_list = [\"normal_finetuning\", \"concreteness_score_addition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining static arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_args = {\n",
    "    'dataset': raw_datasets,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 3e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1728/1728 [00:00<00:00, 66237.35 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 41278.01 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 40477.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training with the following Configurations: {'model_name': 'facebook/bart-base', 'startegy': 'normal_finetuning'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 25%|██▌       | 54/216 [10:08<32:09, 11.91s/it]/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 216/216 [41:31<00:00, 11.53s/it] \n",
      "Parameter 'function'=<bound method ModellingExperiments.tokenize_sentence of <__main__.ModellingExperiments object at 0xffff2b35d9c0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 1728/1728 [00:00<00:00, 141213.00 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 61247.27 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 67303.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training with the following Configurations: {'model_name': 'facebook/bart-base', 'startegy': 'concreteness_score_addition'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 216/216 [28:55<00:00,  8.03s/it]\n",
      "Map: 100%|██████████| 1728/1728 [00:00<00:00, 59813.30 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 36754.82 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 38700.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training with the following Configurations: {'model_name': 'microsoft/deberta-base', 'startegy': 'normal_finetuning'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 25%|██▌       | 54/216 [11:41<35:11, 13.03s/it]/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 216/216 [47:56<00:00, 13.32s/it]\n",
      "Map: 100%|██████████| 1728/1728 [00:00<00:00, 108089.98 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 50466.22 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 54353.83 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training with the following Configurations: {'model_name': 'microsoft/deberta-base', 'startegy': 'concreteness_score_addition'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 25%|██▌       | 54/216 [11:03<32:56, 12.20s/it]/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 50%|█████     | 108/216 [22:12<21:51, 12.15s/it]/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      " 75%|███████▌  | 162/216 [33:36<10:36, 11.80s/it]/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 216/216 [43:40<00:00, 10.04s/it]/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 216/216 [43:55<00:00, 12.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Open file to save results\n",
    "with open('result_dynamic_dict_final.json', 'r') as openfile:\n",
    "    result_dynamic_dict = json.load(openfile)\n",
    "\n",
    "result_list = list()\n",
    "for model_name in model_name_list:\n",
    "    \n",
    "    # Setting Model Name\n",
    "    kw_args[\"model_name\"] = model_name\n",
    "    \n",
    "    # Initializing ModellingExperiments Object\n",
    "    modelling_obj = ModellingExperiments(**kw_args)\n",
    "    \n",
    "    for strategy in strategies_list:\n",
    "        \n",
    "        # Preparing dataset for a specific strategy\n",
    "        modelling_obj.prepare_dataset(strategy=strategy)\n",
    "        \n",
    "        # Preparing data loaders\n",
    "        modelling_obj.prepare_dataloaders()\n",
    "\n",
    "        # Initialize dictionary for storing results\n",
    "        result_dict = {\n",
    "            'model_name': model_name,\n",
    "            'startegy':strategy \n",
    "        }\n",
    "        print(\"Model Training with the following Configurations: {}\".format(result_dict))\n",
    "        \n",
    "        unique_key = \"#\".join(str(i) for i in list(result_dict.values()))\n",
    "        if not result_dynamic_dict.get(unique_key):\n",
    "            result_dynamic_dict[unique_key] = dict()\n",
    "            \n",
    "            # initializing model\n",
    "            modelling_obj.init_model()\n",
    "            \n",
    "            # For a specic num_epochs variable, we are setting up the optimizers\n",
    "            modelling_obj.setup_optimizer(num_epochs=num_epochs)\n",
    "            \n",
    "            # Now, we are training the model\n",
    "            modelling_obj.train_model()\n",
    "            \n",
    "            # Saving evaluated results\n",
    "            evaluation_results_list = modelling_obj.evaluation_results_list\n",
    "            for evaluation_results in evaluation_results_list:\n",
    "                result_dict.update(evaluation_results)\n",
    "                result_list.append(result_dict)\n",
    "                \n",
    "            # Updating the stored file\n",
    "            result_dynamic_dict[unique_key] = evaluation_results_list\n",
    "            \n",
    "            # Storing the updated result file\n",
    "            with open('result_dynamic_dict_final.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result_dynamic_dict, f, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print(\"Model already trained, results are stored already!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = list()\n",
    "for k, v in  result_dynamic_dict.items():\n",
    "    model_name, strategy = k.split(\"#\")\n",
    "    for item in v:\n",
    "        result_dict = dict()\n",
    "        result_dict['model_name'] = model_name\n",
    "        result_dict['strategy'] = strategy\n",
    "        result_dict.update(item)\n",
    "        result_list.append(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>strategy</th>\n",
       "      <th>epoch</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>validation_precision</th>\n",
       "      <th>validation_recall</th>\n",
       "      <th>validation_f1</th>\n",
       "      <th>validation_roc_auc</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook/bart-base</td>\n",
       "      <td>normal_finetuning</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.356481</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416216</td>\n",
       "      <td>0.709887</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.356481</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.416216</td>\n",
       "      <td>0.659824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facebook/bart-base</td>\n",
       "      <td>normal_finetuning</td>\n",
       "      <td>2</td>\n",
       "      <td>0.726852</td>\n",
       "      <td>0.684035</td>\n",
       "      <td>0.543465</td>\n",
       "      <td>0.513718</td>\n",
       "      <td>0.801739</td>\n",
       "      <td>0.689815</td>\n",
       "      <td>0.510989</td>\n",
       "      <td>0.503037</td>\n",
       "      <td>0.459496</td>\n",
       "      <td>0.688940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook/bart-base</td>\n",
       "      <td>normal_finetuning</td>\n",
       "      <td>3</td>\n",
       "      <td>0.768519</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.654587</td>\n",
       "      <td>0.670048</td>\n",
       "      <td>0.794198</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.601173</td>\n",
       "      <td>0.607411</td>\n",
       "      <td>0.709468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>facebook/bart-base</td>\n",
       "      <td>normal_finetuning</td>\n",
       "      <td>4</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>0.703788</td>\n",
       "      <td>0.635212</td>\n",
       "      <td>0.647700</td>\n",
       "      <td>0.793779</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.613247</td>\n",
       "      <td>0.580226</td>\n",
       "      <td>0.583614</td>\n",
       "      <td>0.721093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>facebook/bart-base</td>\n",
       "      <td>concreteness_score_addition</td>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.631965</td>\n",
       "      <td>0.643651</td>\n",
       "      <td>0.748010</td>\n",
       "      <td>0.726852</td>\n",
       "      <td>0.653412</td>\n",
       "      <td>0.610913</td>\n",
       "      <td>0.618797</td>\n",
       "      <td>0.722769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model_name                     strategy  epoch  \\\n",
       "0  facebook/bart-base            normal_finetuning      1   \n",
       "1  facebook/bart-base            normal_finetuning      2   \n",
       "2  facebook/bart-base            normal_finetuning      3   \n",
       "3  facebook/bart-base            normal_finetuning      4   \n",
       "4  facebook/bart-base  concreteness_score_addition      1   \n",
       "\n",
       "   validation_accuracy  validation_precision  validation_recall  \\\n",
       "0             0.712963              0.356481           0.500000   \n",
       "1             0.726852              0.684035           0.543465   \n",
       "2             0.768519              0.727778           0.654587   \n",
       "3             0.754630              0.703788           0.635212   \n",
       "4             0.750000              0.694444           0.631965   \n",
       "\n",
       "   validation_f1  validation_roc_auc  test_accuracy  test_precision  \\\n",
       "0       0.416216            0.709887       0.712963        0.356481   \n",
       "1       0.513718            0.801739       0.689815        0.510989   \n",
       "2       0.670048            0.794198       0.712963        0.632184   \n",
       "3       0.647700            0.793779       0.703704        0.613247   \n",
       "4       0.643651            0.748010       0.726852        0.653412   \n",
       "\n",
       "   test_recall   test_f1  test_roc_auc  \n",
       "0     0.500000  0.416216      0.659824  \n",
       "1     0.503037  0.459496      0.688940  \n",
       "2     0.601173  0.607411      0.709468  \n",
       "3     0.580226  0.583614      0.721093  \n",
       "4     0.610913  0.618797      0.722769  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(result_list)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../../results/FinalResultsPAP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
