{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAP (Binary) - Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Loads PAP datasets\n",
    "datasets_path = '../../datasets/pap/train-dev-test-split/binary'\n",
    "train_df = pd.read_csv(f'{datasets_path}/train.csv')\n",
    "dev_df = pd.read_csv(f'{datasets_path}/dev.csv')\n",
    "test_df = pd.read_csv(f'{datasets_path}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads and preprocess concreteness ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got this dataset for concreteness of 40k words (https://pubmed.ncbi.nlm.nih.gov/24142837/) from https://web.stanford.edu/class/linguist278/data/\n",
    "# Load concreteness ratings\n",
    "concreteness_df = pd.read_csv('../../datasets/concreteness/Concreteness_ratings_Brysbaert_et_al_BRM.csv')\n",
    "concreteness_df.head(2)\n",
    "\n",
    "# Map and normalize conreteness ratings\n",
    "word_to_concreteness_score_map = dict()\n",
    "for idx, row in concreteness_df.iterrows():\n",
    "    row = row.to_dict()\n",
    "    \n",
    "    # Normalizing to a scale of 0 to 1\n",
    "    word_to_concreteness_score_map[row['Word']] = row['Conc.M']/5.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions to get concreteness scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_concreteness_score(word):\n",
    "    \"\"\"\n",
    "    Get the concreteness score of a word based on the Concreteness Ratings dataset.\n",
    "    \"\"\"\n",
    "    # If the word is not found in the dataset, return a default score of 0.5\n",
    "    return round(word_to_concreteness_score_map.get(word, 0.5), 3)\n",
    "\n",
    "def calculate_text_concreteness_sequence(text):\n",
    "    \"\"\"\n",
    "    Calculate the concreteness score for a given text.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    concreteness_scores = [get_concreteness_score(word) for word in words]\n",
    "    concreteness_scores = \" \".join([str(i) for i in concreteness_scores])\n",
    "    # Take the average concreteness score of all words in the text\n",
    "    return concreteness_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PAP datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add concreteness scores for the every sequence\n",
    "train_df['concreteness_score_sequence'] = train_df.text.apply(calculate_text_concreteness_sequence)\n",
    "dev_df['concreteness_score_sequence'] = dev_df.text.apply(calculate_text_concreteness_sequence)\n",
    "test_df['concreteness_score_sequence'] = test_df.text.apply(calculate_text_concreteness_sequence)\n",
    "\n",
    "# Load PAP datasets with Concreteness Scores  \n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(dev_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_KEEP = ['label', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "class ModellingExperiments:\n",
    "    \n",
    "    def __init__(self, model_name, dataset, batch_size, learning_rate):\n",
    "        self.model_name = model_name\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.cols_to_keep = set(COLUMNS_TO_KEEP)\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.data_collator = DataCollatorWithPadding(self.tokenizer)\n",
    "\n",
    "    def init_model(self):\n",
    "        torch.manual_seed(12)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def tokenize_sentence_with_concreteness_score(self, item):\n",
    "        # We also tried using the concreteness score for the whole sentence as a feature input\n",
    "        # To implement that, we changed the source code of transformers library and changed the classification head manually \n",
    "        # So that we can accomodate that extra feature, but this method of encoding concreteness score sequence was giving better score\n",
    "        # Hence, we are using this only in the final experiments. You can check that experiment out in the following notebook:\n",
    "        # modelling/pap/experiments/FinalModellingWithConcretenessScore[BERT] - PAP\n",
    "        return self.tokenizer(item['text'], item['concreteness_score_sequence'], truncation=True)\n",
    "        \n",
    "    def tokenize_sentence(self, item):\n",
    "        # Normal tokenization\n",
    "        return self.tokenizer(item['text'], truncation=True)\n",
    "        \n",
    "    def add_strategy_to_tokenizer_function_map(self):\n",
    "        # Mapping between strategy and the tokenization functions defined above\n",
    "        # Strategy refers to whether we are using normal tokenization or whether we want to do paired tokenization of \n",
    "        # both input sentence and the sequence of concreteness score for that sentence\n",
    "        self.strategy_to_tokenizer_function_map = dict()\n",
    "        self.strategy_to_tokenizer_function_map['normal_finetuning'] = self.tokenize_sentence_with_concreteness_score\n",
    "        self.strategy_to_tokenizer_function_map['concreteness_score_addition'] = self.tokenize_sentence\n",
    "        \n",
    "    def prepare_dataset(self, strategy):\n",
    "        # Here, we wull tokenize the dataset based on the strategy we are planning to use\n",
    "        self.strategy = strategy\n",
    "        self.add_strategy_to_tokenizer_function_map()\n",
    "        self.tokenized_dataset = self.dataset.map(self.strategy_to_tokenizer_function_map[self.strategy], batched=True)\n",
    "        current_cols = set(list(self.tokenized_dataset['train'].features.keys()))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.remove_columns(list(current_cols - self.cols_to_keep))\n",
    "        self.tokenized_dataset = self.tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "        self.tokenized_dataset = self.tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "    def prepare_dataloaders(self):\n",
    "        self.train_dataloader = DataLoader(self.tokenized_dataset['train'], batch_size=self.batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "        self.validation_dataloader = DataLoader(self.tokenized_dataset['validation'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "        self.test_dataloader = DataLoader(self.tokenized_dataset['test'], batch_size=self.batch_size, collate_fn=self.data_collator)\n",
    "\n",
    "    def setup_optimizer(self, num_epochs):\n",
    "        # Setting up optimizer and learning rate scheduler\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.num_training_steps = self.num_epochs*len(self.train_dataloader)\n",
    "        self.learning_rate_scheduler = get_scheduler(\"linear\", optimizer=self.optimizer, num_warmup_steps=0, num_training_steps=self.num_training_steps)\n",
    "    \n",
    "    def train_model(self):\n",
    "        # Training the Model\n",
    "        self.evaluation_results_list = list()\n",
    "        progress_bar = tqdm(range(self.num_training_steps))\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            for batch in self.train_dataloader:\n",
    "                batch = {k:v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                # calculating gradients\n",
    "                loss.backward()\n",
    "                # optimizing weights\n",
    "                self.optimizer.step()\n",
    "                # updating learning rate\n",
    "                self.learning_rate_scheduler.step()\n",
    "                # flushing gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # updating progress bar\n",
    "                progress_bar.update(1)\n",
    "            # evaluating per epoch\n",
    "            self.eval_model(self.validation_dataloader)\n",
    "            eval_results = dict()\n",
    "            eval_results['epoch'] = epoch\n",
    "            for k, v in self.eval_dict.items():\n",
    "                eval_results[\"validation_{}\".format(k)] = v\n",
    "            self.eval_model(self.test_dataloader)\n",
    "            for k, v in self.eval_dict.items():\n",
    "                eval_results[\"test_{}\".format(k)] = v\n",
    "            self.evaluation_results_list.append(eval_results)\n",
    "\n",
    "    def initialize_metrics(self):\n",
    "        self.metrics = {\n",
    "            'accuracy': evaluate.load('accuracy'),\n",
    "            'precision': evaluate.load('precision'),\n",
    "            'recall': evaluate.load('recall'),\n",
    "            'f1': evaluate.load('f1'),\n",
    "            'roc-auc': evaluate.load(\"roc_auc\"),\n",
    "            #'confusion-matrix': evaluate.load(\"BucketHeadP65/confusion_matrix\"),\n",
    "            #'roc-curve': evaluate.load(\"BucketHeadP65/roc_curve\"),\n",
    "        }\n",
    "                \n",
    "    def eval_model(self, dataloader):\n",
    "        \n",
    "        # Evaluating the model on different dataloaders\n",
    "        self.initialize_metrics()\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Move batch data to the specified device (GPU or CPU)\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "    \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch)\n",
    "            \n",
    "            # Extract logits and predictions\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Extract probabilities for the positive class\n",
    "            positive_probabilities = probabilities[:, 1].to(self.device).numpy()\n",
    "        \n",
    "            # Update metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "            self.metrics['accuracy'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['precision'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['recall'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['f1'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            self.metrics['roc-auc'].add_batch(prediction_scores=positive_probabilities, references=batch['labels'])\n",
    "            #self.metrics['confusion-matrix'].add_batch(predictions=predictions, references=batch['labels'])\n",
    "            #self.metrics['roc-curve'].add_batch(prediction_scores=positive_probabilities, references=batch['labels'])\n",
    "        \n",
    "        # Compute metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "        self.eval_dict = {}\n",
    "        self.eval_dict.update(self.metrics['accuracy'].compute())\n",
    "        self.eval_dict.update(self.metrics['precision'].compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.metrics['recall'].compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.metrics['f1'].compute(average=\"macro\"))\n",
    "        self.eval_dict.update(self.metrics['roc-auc'].compute(average=\"macro\"))\n",
    "        #self.eval_dict.update(self.metrics['confusion-matrix'].compute()) \n",
    "        #self.eval_dict.update(self.metrics['roc-curve'].compute())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining parameters on which we will run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "model_name_list = [\"facebook/bart-base\", \"microsoft/deberta-base\"]\n",
    "num_epochs = 4\n",
    "strategies_list = [\"normal_finetuning\", \"concreteness_score_addition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining static arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_args = {\n",
    "    'dataset': raw_datasets,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 3e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1728/1728 [00:00<00:00, 74722.23 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 50342.84 examples/s]\n",
      "Map: 100%|██████████| 216/216 [00:00<00:00, 48595.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training with the following Configurations: {'model_name': 'facebook/bart-base', 'startegy': 'normal_finetuning'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/216 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 14%|█▍        | 31/216 [04:36<26:39,  8.65s/it]"
     ]
    }
   ],
   "source": [
    "# Open file to save results\n",
    "with open('result_dynamic_dict_final.json', 'r') as openfile:\n",
    "    result_dynamic_dict = json.load(openfile)\n",
    "\n",
    "result_list = list()\n",
    "for model_name in model_name_list:\n",
    "    \n",
    "    # Setting Model Name\n",
    "    kw_args[\"model_name\"] = model_name\n",
    "    \n",
    "    # Initializing ModellingExperiments Object\n",
    "    modelling_obj = ModellingExperiments(**kw_args)\n",
    "    \n",
    "    for strategy in strategies_list:\n",
    "        \n",
    "        # Preparing dataset for a specific strategy\n",
    "        modelling_obj.prepare_dataset(strategy=strategy)\n",
    "        \n",
    "        # Preparing data loaders\n",
    "        modelling_obj.prepare_dataloaders()\n",
    "\n",
    "        # Initialize dictionary for storing results\n",
    "        result_dict = {\n",
    "            'model_name': model_name,\n",
    "            'startegy':strategy \n",
    "        }\n",
    "        print(\"Model Training with the following Configurations: {}\".format(result_dict))\n",
    "        \n",
    "        unique_key = \"#\".join(str(i) for i in list(result_dict.values()))\n",
    "        if not result_dynamic_dict.get(unique_key):\n",
    "            result_dynamic_dict[unique_key] = dict()\n",
    "            \n",
    "            # initializing model\n",
    "            modelling_obj.init_model()\n",
    "            \n",
    "            # For a specic num_epochs variable, we are setting up the optimizers\n",
    "            modelling_obj.setup_optimizer(num_epochs=num_epochs)\n",
    "            \n",
    "            # Now, we are training the model\n",
    "            modelling_obj.train_model()\n",
    "            \n",
    "            # Saving evaluated results\n",
    "            evaluation_results_list = modelling_obj.evaluation_results_list\n",
    "            for evaluation_results in evaluation_results_list:\n",
    "                result_dict.update(evaluation_results)\n",
    "                result_list.append(result_dict)\n",
    "                \n",
    "            # Updating the stored file\n",
    "            result_dynamic_dict[unique_key] = evaluation_results_list\n",
    "            \n",
    "            # Storing the updated result file\n",
    "            with open('result_dynamic_dict_final.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result_dynamic_dict, f, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print(\"Model already trained, results are stored already!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(result_list)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('../../results/FinalResultsPAP.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
