{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c35b670",
   "metadata": {},
   "source": [
    "# PAP with Concreteness Ratings - Fine-tuning a BERT based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb1335",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28baba-703d-46b5-9083-d6a2a801d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from transformers import DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "# Loads PAP datasets\n",
    "datasets_path = '../../../datasets/pap/train-dev-test-split/binary'\n",
    "train_df = pd.read_csv(f'{datasets_path}/train.csv')\n",
    "dev_df = pd.read_csv(f'{datasets_path}/dev.csv')\n",
    "test_df = pd.read_csv(f'{datasets_path}/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19230b30",
   "metadata": {},
   "source": [
    "### Loads and preprocess concreteness ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c069107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got this dataset for concreteness of 40k words (https://pubmed.ncbi.nlm.nih.gov/24142837/) from https://web.stanford.edu/class/linguist278/data/\n",
    "# Load concreteness ratings\n",
    "concreteness_df = pd.read_csv('../../../datasets/concreteness/Concreteness_ratings_Brysbaert_et_al_BRM.csv')\n",
    "concreteness_df.head(2)\n",
    "\n",
    "# Map and normalize conreteness ratings\n",
    "word_to_concreteness_score_map = dict()\n",
    "for idx, row in concreteness_df.iterrows():\n",
    "    row = row.to_dict()\n",
    "    \n",
    "    # Normalizing to a scale of 0 to 1\n",
    "    word_to_concreteness_score_map[row['Word']] = row['Conc.M']/5.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050fcb3",
   "metadata": {},
   "source": [
    "Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fad5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concreteness_score(word):\n",
    "    \"\"\"\n",
    "    Get the concreteness score of a word based on the Concreteness Ratings dataset.\n",
    "    \"\"\"\n",
    "    # If the word is not found in the dataset, return a default score of 0.5\n",
    "    return round(word_to_concreteness_score_map.get(word, 0.5), 3)\n",
    "\n",
    "def calculate_text_concreteness_sequence(text):\n",
    "    \"\"\"\n",
    "    Calculate the concreteness score for a given text.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    concreteness_scores = [get_concreteness_score(word) for word in words]\n",
    "    concreteness_scores = \" \".join([str(i) for i in concreteness_scores])\n",
    "    # Take the average concreteness score of all words in the text\n",
    "    return concreteness_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5fdfe",
   "metadata": {},
   "source": [
    "Add the sequence concreteness score to the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add concreteness scores for the every sequence\n",
    "train_df['concreteness_score_sequence'] = train_df.text.apply(calculate_text_concreteness_sequence)\n",
    "dev_df['concreteness_score_sequence'] = dev_df.text.apply(calculate_text_concreteness_sequence)\n",
    "test_df['concreteness_score_sequence'] = test_df.text.apply(calculate_text_concreteness_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a645e70-ca3e-4d9e-b0ac-ab2aaaedd309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PAP datasets with Concreteness Scores  \n",
    "raw_datasets = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(dev_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "# Initialize model's parameters\n",
    "model_parameters = {\n",
    "    'learning_rate': 3e-5,\n",
    "    'epochs': 3,\n",
    "    'model_name': 'bert-base-uncased',\n",
    "}\n",
    "checkpoint = model_parameters['model_name']\n",
    "\n",
    "# Init pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Simple tokenize function\n",
    "def tokenize_function(dataset, truncation=True):\n",
    "    return tokenizer(dataset['text'], dataset['concreteness_score_sequence'])\n",
    "\n",
    "# Tokenize raw data\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Clear datasets dictionary\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['text', 'original_label', 'concreteness_score_sequence'])\n",
    "tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "tokenized_datasets.set_format('torch')\n",
    "\n",
    "# Initialize the data collator with padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# DEBUG\n",
    "print('raw datasets:\\n', raw_datasets, '\\n')\n",
    "print('tokenized datasets:\\n', tokenized_datasets)\n",
    "\n",
    "\n",
    "# Initialize DataLoader for train, validation and test splits\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55409b66",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea312379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a pre-trained BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_parameters['model_name'], num_labels=2)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=model_parameters['learning_rate'])\n",
    "\n",
    "# Run on GPU available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize scheduler\n",
    "num_training_steps = model_parameters['epochs'] * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Display progress bar\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(model_parameters['epochs']):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # calculating gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # optimizing weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # updating learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # flushing gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # updating progress bar\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f48c20",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463595d",
   "metadata": {},
   "source": [
    "Evaluate on development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d606f2-8d07-424e-8764-fa8c7afafd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on GPU available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Define evaluation metrics\n",
    "accuracy = evaluate.load('accuracy')\n",
    "precision = evaluate.load('precision')\n",
    "recall = evaluate.load('recall')\n",
    "f1 = evaluate.load('f1')\n",
    "roc_auc =  evaluate.load(\"roc_auc\")\n",
    "\n",
    "# Set model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    \n",
    "    # Move batch data to the specified device (GPU or CPU)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    # Extract logits and predictions\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Extract probabilities for the positive class\n",
    "    positive_probabilities = probabilities[:, 1].to(device).numpy()\n",
    "\n",
    "    # Update metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "    accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    precision.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    recall.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    f1.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    roc_auc.add_batch(prediction_scores=positive_probabilities, references=batch['labels'])\n",
    "\n",
    "# Compute metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "validation_eval_dict = {}\n",
    "validation_eval_dict.update(accuracy.compute())\n",
    "validation_eval_dict.update(precision.compute(average='macro'))\n",
    "validation_eval_dict.update(recall.compute(average='macro'))\n",
    "validation_eval_dict.update(f1.compute(average='macro'))\n",
    "validation_eval_dict.update(roc_auc.compute(average='macro'))\n",
    "\n",
    "# Print evaluation metrics\n",
    "validation_eval_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af5766",
   "metadata": {},
   "source": [
    "Predictions on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f089824-0bc6-4d72-9167-cf0e5e4ea750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define evaluation metrics\n",
    "accuracy = evaluate.load('accuracy')\n",
    "precision = evaluate.load('precision')\n",
    "recall = evaluate.load('recall')\n",
    "f1 = evaluate.load('f1')\n",
    "roc_auc =  evaluate.load(\"roc_auc\")\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Move batch data to the specified device (GPU or CPU)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    # Extract logits and predictions\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Extract probabilities for the positive class\n",
    "    positive_probabilities = probabilities[:, 1].to(device).numpy()\n",
    "\n",
    "    # Update metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "    accuracy.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    precision.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    recall.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    f1.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    roc_auc.add_batch(prediction_scores=positive_probabilities, references=batch['labels'])\n",
    "\n",
    "# Compute metrics for accuracy, precision, recall, F1 and ROC-AUC\n",
    "test_eval_dict = {}\n",
    "test_eval_dict.update(accuracy.compute())\n",
    "test_eval_dict.update(precision.compute(average='macro'))\n",
    "test_eval_dict.update(recall.compute(average='macro'))\n",
    "test_eval_dict.update(f1.compute(average='macro'))\n",
    "test_eval_dict.update(roc_auc.compute(average='macro'))\n",
    "\n",
    "# Print evaluation metrics\n",
    "test_eval_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
