#pip install --upgrade pip
#python3 -m pip install -r ../requirements.txt
#import nltk
#nltk.download('all')


!ls ../datasets/pap/train-dev-test-split/binary





import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

PAP_PATH = '../datasets/pap/train-dev-test-split/binary'
train_file = f'{PAP_PATH}/train.csv'
dev_file = f'{PAP_PATH}/dev.csv'
test_file = f'{PAP_PATH}/test.csv'

train_df = pd.read_csv(train_file)
dev_df = pd.read_csv(dev_file)
test_df = pd.read_csv(test_file)


df = pd.concat([train_df, dev_df, test_df])
df = df.rename(columns={'original_label': 'category'})

df['nn_1'] = [sent.split()[0] for sent in df['text']]
df['v'] = [sent.split()[1] for sent in df['text']]
df['nn_2'] = [sent.split()[2] for sent in df['text']]

df.head(2)








text_data_all = '\n'.join(df['text']) # using \n character as the delimiter between texts in each row.
# Create the WordCloud object
wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(text_data_all)

# Display the WordCloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()





text_data_train = '\n'.join(train_df['text']) # using \n character as the delimiter between texts in each row.
# Create the WordCloud object
wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(text_data_train)

# Display the WordCloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()





text_data_dev = '\n'.join(dev_df['text']) # using \n character as the delimiter between texts in each row.
# Create the WordCloud object
wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(text_data_dev)

# Display the WordCloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()





text_data_test = '\n'.join(test_df['text']) # using \n character as the delimiter between texts in each row.
# Create the WordCloud object
wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110).generate(text_data_test)

# Display the WordCloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()








# Count the occurrences of each category
category_counts = df['category'].value_counts()

# Plot the distribution using a bar plot
plt.figure(figsize=(8, 5))
category_counts.plot(kind='bar', color='skyblue')
plt.title('Distribution of Categories')
plt.xlabel('Categories')
plt.ylabel('Count')
plt.show()












import seaborn as sns

# Tokenize the string into words
# Not needed
words = word_tokenize(text_data_all)

# Count the occurrences of each word
word_counts = Counter(words)

sns.histplot(word_counts).set(xlabel='# a word has been seen')
plt.show()







import seaborn as sns

# Tokenize the string into words
# Not needed
words = word_tokenize(text_data_train)

# Count the occurrences of each word
word_counts = Counter(words)

sns.histplot(word_counts).set(xlabel='# a word has been seen')
plt.show()







import seaborn as sns

# Tokenize the string into words
# Not needed
words = word_tokenize(text_data_dev)

# Count the occurrences of each word
word_counts = Counter(words)

sns.histplot(word_counts).set(xlabel='# a word has been seen')
plt.show()










import seaborn as sns

# Tokenize the string into words
# Not needed
words = word_tokenize(text_data_test)

# Count the occurrences of each word
word_counts = Counter(words)

sns.histplot(word_counts).set(xlabel='# a word has been seen')
plt.show()










import numpy as np
import seaborn as sns

# Tokenize the text into words
words = word_tokenize(text_data_all)

# Calculate the length of each word
word_lengths = [len(word) for word in words]
# Compute the average word length
average_word_length = np.mean(word_lengths)

plt.figure(figsize=(8, 6))
plt.hist(word_lengths, color='skyblue', edgecolor='black')
plt.xlabel('Word Length')
plt.ylabel('Frequency')
plt.title('Distribution of Word Lengths')
plt.show()








BRM_PATH = '../datasets/brm'
conc_file = f'{BRM_PATH}/Concreteness_ratings_Brysbaert_et_al_BRM.txt'
conc_df = pd.read_csv(conc_file, sep="\t")
conc_df.head(2)





word_to_concreteness_score_map = dict()
for idx, row in conc_df.iterrows():
    row = row.to_dict()
    word_to_concreteness_score_map[row['Word']] = row['Conc.M']/5.0 # Normalizing to a scale of 0 to 1

def get_concreteness_score(word):
    """
    Get the concreteness score of a word based on the Concreteness Ratings dataset.
    """
    # If the word is not found in the dataset, return a default score of 0.5
    return word_to_concreteness_score_map.get(word, 0.5)

def calculate_text_concreteness(text):
    """
    Calculate the concreteness score for a given text.
    """
    words = nltk.word_tokenize(text)
    concreteness_scores = [get_concreteness_score(word) for word in words]
    # Take the average concreteness score of all words in the text
    return sum(concreteness_scores) / len(concreteness_scores)

def calculate_nns_concreteness(text):
    """
    Calculate concreteness score for NNs
    """
    words = nltk.word_tokenize(text)
    nn1, v, nn2 = words[0], words[1], words[2]
    concreteness_scores = [get_concreteness_score(nn1), get_concreteness_score(nn2)]
    # Take the average concreteness score of all NNs in the text
    return sum(concreteness_scores) / len(concreteness_scores)



df['text_conc_score'] = df.text.apply(calculate_text_concreteness)
df['conc_nn_1'] = df.nn_1.apply(get_concreteness_score)
df['conc_v'] = df.v.apply(get_concreteness_score)
df['conc_nn_2'] = df.nn_2.apply(get_concreteness_score)
df['conc_nns'] = df.text.apply(calculate_nns_concreteness)

df.head(4)


import seaborn as sns

fig, ax = plt.subplots(figsize=(12, 4))
sns.kdeplot(data=df, x='conc_nn_1', hue='category', ax=ax)





import seaborn as sns

fig, ax = plt.subplots(figsize=(12, 4))
sns.kdeplot(data=df, x='conc_nn_2', hue='category', ax=ax)





import seaborn as sns

fig, ax = plt.subplots(figsize=(12, 4))
sns.kdeplot(data=df, x='conc_v', hue='category', ax=ax)





import seaborn as sns

fig, ax = plt.subplots(figsize=(12, 4))
sns.kdeplot(data=df, x='text_conc_score', hue='category', ax=ax)








import seaborn as sns

fig, ax = plt.subplots(figsize=(12, 4))
sns.kdeplot(data=df, x='conc_nns', hue='category', ax=ax)


import seaborn as sns

sns.boxplot(df, x='text_conc_score', y='category')
plt.show()


import seaborn as sns

sns.boxplot(df, x='conc_nn_1', y='category')
plt.show()


import seaborn as sns

sns.boxplot(df, x='conc_nn_2', y='category')
plt.show()
